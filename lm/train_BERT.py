# -*- coding: utf-8 -*-
"""CharBERTV3_BASE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOlk2rIdrl2_LSFyi1WZUa7Sc9Kn94qs


## Next step:
    # Why wasn't epoch loop working?
    # yhat vs ytruth, check for correctness

"""

# !pip install git+https://github.com/huggingface/transformers
# conda activate transformers
# from google.colab import drive
# drive.mount('/content/drive')
# import os
# os.chdir("/content/drive/My Drive/internn/data")
import torch.optim as optim
import copy
import numpy as np
from error_measures import *
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import os
from sen_loader import *
from transformers import BertPreTrainedModel
from transformers.models.bert.modeling_bert import *
import sys
from CustomBert import *
sys.path.append("..")
from pytorch_utils import *
from general_tools.my_logging import *
import wandb
import random
from transformers import AdamW


WANDB = True
if WANDB:
    wandb.init(project="TrainBERT")

ROOT = get_root("internn")
print(ROOT)
PATH = ROOT / "data/embedding_datasets/embeddings_v2.1"

epochs = 500
starting_epoch = 0
lr = 1e-4
losses = []
losses_10 = []
vocab_size = 27
device = torch.device('cuda:0')
mask_one_token = True
text = 'abcdefghi jklmnopqrstuvwxyz'
corpus = [char for char in text]
mask_id = len(corpus) + 1
vocab_size = mask_id + 2 # some special characters?
batch_size = 192
embedding_dim = 512

config = {
    "epochs" : epochs,
    "starting_epoch" : starting_epoch,
    "lr" : lr,
    "losses" : losses,
    "losses_10" : losses_10,
    "vocab_size" : vocab_size,
    "device" : device,
    "mask_one_token": mask_one_token,
    "text" : text,
    "corpus" : corpus,
    "mask_id" : mask_id,
    "vocab_size" : vocab_size,
    "batch_size" : batch_size,
    "embedding_dim" : 512
}

if WANDB:
    wandb.config = config

EXPERIMENT_NAME = "BERT_embedding*.pt"
#EXPERIMENT_NAME = "BERT_logit*.pt"

train_dataset = SentenceDataset(PATH=PATH / 'train_test_sentenceDataset.pt', which='Embeddings')
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                                           collate_fn=collate_fn_embeddings,
                                           num_workers=12)

test_dataset = SentenceDataset(PATH=PATH / 'train_test_sentenceDataset.pt', which='Embeddings', train=False)
test_loader = torch.utils.data.DataLoader(train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True,
                                          collate_fn=collate_fn_embeddings,
                                          num_workers=3)

sample = next(iter(train_dataset))
print("Batch Data Shape = ", sample["data"].squeeze(0).shape)
print("Num Batch Labels = ", sample["gt_one_hot"].squeeze(0).shape)
print("Output shape: ", sample["vgg_logits"].shape)
print("Sen Lengths: ", sample["length"])

print(get_text(sample["gt_one_hot"]))

model = BertModelCustom(BertConfig(vocab_size=vocab_size + 2,
                                   hidden_size=embedding_dim,
                                   num_attention_heads=8)).to(device)
objective = nn.CrossEntropyLoss()
#objective = nn.NLLLoss(ignore_index=0)

optimizer = AdamW(model.parameters(), lr=lr)
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=40, factor=.5)

if PATH:
    latest = get_latest_file(PATH, EXPERIMENT_NAME)
    if latest:
        print(f"Loading {latest}")
        old_state = load_model(latest, model, optimizer, scheduler)
        #old_state = load_model(latest, model, optimizer=None)
        loss = old_state["loss"]
        epoch = old_state["epoch"]

## TEST MODEL
input_ids, attention_mask, labels, mask_index = get_inputs("my name is sam", corpus=corpus, mask_id=mask_id)
output = model(input_ids.to(device), attention_mask.to(device))

prev_input = None
cur_input = None
prev_loss = 100
cur_loss = 100
prev_output = None
cur_output = None
val_loss = []
count = 0

X_KEY = "vgg_logits" if "logit" in EXPERIMENT_NAME else "data"

wtd_sum = 0
wt = 0
cer_list = []

train_dataset.train_mode = "full sequence"

def cer_calculation(sample,output):
    wtd_sum = 0;wt = 0
    ### CER CALCULATION
    text = [s.lower() for s in sample["text"]]
    wt += np.array(sample["length"]).sum();
    out_text = [get_text(o.argmax(-1))[:sample["length"][i]] for i, o in enumerate(output)];
    for i, t in enumerate(text):
        wtd_sum += cer(text[i], out_text[i]) * sample["length"][i]
    print(out_text[0], ";", text[0])
    _cer = wtd_sum / wt
    print("CER", _cer)
    return _cer

lr = optimizer.param_groups[0]['lr']
print("INITIAL LR: ", lr)

for epoch in range(starting_epoch, epochs):
    print("epoch", epoch)
    for sample in train_loader:
        # train_dataset.train_mode = "single character" if random.random() < .5 else "full sequence"

        x, y_truth, attention_mask = sample[X_KEY].to(device), sample["masked_gt"].to(device), sample["attention_mask"].to(device)

        attention_mask = attention_mask.to(device) # 1, sentence_length
        optimizer.zero_grad()
        if True:
            output, y_hat = model(inputs_embeds=x, attention_mask=attention_mask)
        else:
            # If just training language model
            # input_ids = sample["gt_idxs"].to(device)
            # output, y_hat = model(input_ids=input_ids, attention_mask=attention_mask)
            pass
        loss = objective(output.view(-1, vocab_size), y_truth.squeeze(0).view(-1).to(device)) # y_hat includes extra tokens?
        loss.backward()
        losses_10.append(loss.item())
        optimizer.step()


        #get_text(output.squeeze())
        #sample["text"]

        count = count + 1
        if count % 10 == 0 or count < 10:
            l = np.average(losses_10)
            losses.append(l)
            losses_10 = []
            print("count", count, l)
            cer_list.append(cer_calculation(sample,output))
            scheduler.step(l)
            lr = optimizer.param_groups[0]['lr']
            if lr < config["lr"]:
                print("New LR:", lr)
                config["lr"] = lr

    if epoch % 10 == 0:
        save_model(incrementer(PATH, EXPERIMENT_NAME), model, optimizer, epoch=epoch+1, loss=losses[-1] if losses else 0, scheduler=scheduler)

import matplotlib.pyplot as plt
plt.plot(losses)
plt.show()
print(losses)

total = 0
correct = 0

"""
Mask one letter and predict it
"""
for i_batch, sample in enumerate(train_loader):
    x, y_truth = sample["data"].to(device), sample["gt"].to(device)

    text = get_text(y_truth.squeeze(0))
    input_ids, attention_mask, y_truth, index = get_inputs(text)

    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    output, y_hat = model(input_ids, attention_mask)

    if (text[index] == get_text(y_hat.argmax(-1))):
        correct = correct + 1

    total = total + 1

print(correct / total)

# correct = 0
# for x, y_truth in val_loader:
#   x, y_truth = x.to(device), y_truth.to(device)
#   output, y_hat = model(input_embeddings = x)
#   correct = correct + int(torch.sum(output.argmax(-1).squeeze(0) ==  y_truth.argmax(-1)))

# print(correct/len(val_loader.dataset))

total = 0
correct = 0
for i_batch, sample in enumerate(test_loader):
    x, y_truth = sample["data"].to(device), sample["gt"].to(device)

    text = get_text(y_truth.squeeze(0))
    input_ids, attention_mask, y_truth, index = get_inputs(text)

    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    output, y_hat = model(input_ids, attention_mask)

    # print("Text: ", text, index)
    # print("Label: ", text[index])
    # print("Prediction: ", get_text(y_truth.argmax(-1)))

    if (text[index] == get_text(y_truth.argmax(-1))):
        correct = correct + 1

    total = total + 1

print(correct / total)
