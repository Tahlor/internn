# -*- coding: utf-8 -*-
"""CharBERTV3_BASE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOlk2rIdrl2_LSFyi1WZUa7Sc9Kn94qs

## Next step:
    # Why wasn't epoch loop working?
    # yhat vs ytruth, check for correctness

"""

# !pip install git+https://github.com/huggingface/transformers
# conda activate transformers
# from google.colab import drive
# drive.mount('/content/drive')
# import os
# os.chdir("/content/drive/My Drive/internn/data")
import torch.optim as optim
import copy
import numpy as np
from error_measures import *
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import os
from sen_loader import *
from transformers import BertPreTrainedModel
from transformers.models.bert.modeling_bert import *
import sys
from CustomBert import *
sys.path.append("..")
from pytorch_utils import *
from general_tools.my_logging import *
import wandb
import random
from transformers import AdamW
import process_config_BERT
from lm_utils import *

config = process_config_BERT.process_config(sys.argv[0])

if config.wandb:
    wandb.config = config
    wandb.init(project="TrainBERT")

ROOT = get_root("internn")

LOADER_PATH = ROOT / config.folder_dependencies
MODEL_PATH = ROOT / config.folder_outputs
losses = []
losses_10 = []
text = 'abcdefghi jklmnopqrstuvwxyz'
corpus = [char for char in text]

EXPERIMENT_NAME = config.experiment_prefix + config.experiment_type

train_dataset = SentenceDataset(PATH=LOADER_PATH / 'train_test_sentenceDataset.pt', which='Embeddings')
train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=config.batch_size,
                                           shuffle=True,
                                           collate_fn=collate_fn_embeddings,
                                           num_workers=12)


sample = next(iter(train_dataset))
print("Batch Data Shape = ", sample["data"].squeeze(0).shape)
print("Num Batch Labels = ", sample["gt_one_hot"].squeeze(0).shape)
print("Output shape: ", sample["vgg_logits"].shape)
print("Sen Lengths: ", sample["length"])

print(get_text(sample["gt_one_hot"]))

model = BertModelCustom(BertConfig(vocab_size=config.vocab_size_extended,
                                   hidden_size=config.experiment.embedding_dim,
                                   num_attention_heads=config.experiment.attention_heads)).to(config.device)
objective = nn.CrossEntropyLoss()
#objective = nn.NLLLoss(ignore_index=0)

optimizer = AdamW(model.parameters(), lr=config.lr)
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=config.patience, factor=config.decay_factor)

latest = get_latest_file(config.folder_outputs, EXPERIMENT_NAME)
if latest:
    print(f"Loading {latest}")
    old_state = load_model(latest, model, optimizer, scheduler)
    config.starting_epoch, loss = old_state["epoch"], old_state["loss"]

## TEST MODEL
input_ids, attention_mask, labels, mask_index = get_inputs("my name is sam", corpus=corpus, mask_id=config.mask_id)
output = model(input_ids.to(config.device), attention_mask.to(config.device))

prev_input = None
cur_input = None
prev_loss = 100
cur_loss = 100
prev_output = None
cur_output = None
val_loss = []
count = 0

wtd_sum = 0
wt = 0
cer_list = []

train_dataset.train_mode = "full sequence"

lr = optimizer.param_groups[0]['lr']
print("INITIAL LR: ", lr)

for epoch in range(config.starting_epoch if config.starting_epoch else 0, config.epochs):
    print("epoch", epoch)
    for sample in train_loader:
        # train_dataset.train_mode = "single character" if random.random() < .5 else "full sequence"

        x, y_truth, attention_mask = sample[config.experiment.loader_key].to(config.device), sample["masked_gt"].to(config.device), sample["attention_mask"].to(config.device)

        attention_mask = attention_mask.to(config.device) # 1, sentence_length
        optimizer.zero_grad()
        if True:
            output, y_hat = model(inputs_embeds=x, attention_mask=attention_mask)
        else:
            # If just training language model
            # input_ids = sample["gt_idxs"].to(config.device)
            # output, y_hat = model(input_ids=input_ids, attention_mask=attention_mask)
            pass
        loss = objective(output.view(-1, config.vocab_size_extended), y_truth.squeeze(0).view(-1).to(config.device)) # y_hat includes extra tokens?
        loss.backward()
        losses_10.append(loss.item())
        optimizer.step()


        #get_text(output.squeeze())
        #sample["text"]

        count = count + 1
        if count % config.steps_per_lr_update == 0 or count < 10:
            l = np.average(losses_10)
            losses.append(l)
            losses_10 = []
            print("count", count, l)
            cer_list.append(cer_calculation(sample,output))
            scheduler.step(l)
            lr = optimizer.param_groups[0]['lr']
            if lr < config["lr"]:
                print("New LR:", lr)
                config["lr"] = lr

    if epoch % config.save_freq_epoch == 0:
        save_model(incrementer(MODEL_PATH, EXPERIMENT_NAME), model, optimizer, epoch=epoch+1, loss=losses[-1] if losses else 0, scheduler=scheduler)

import matplotlib.pyplot as plt
plt.plot(losses)
plt.show()
print(losses)

total = 0
correct = 0

"""
Mask one letter and predict it
"""
for i_batch, sample in enumerate(train_loader):
    x, y_truth = sample["data"].to(config.device), sample["gt"].to(config.device)

    text = get_text(y_truth.squeeze(0))
    input_ids, attention_mask, y_truth, index = get_inputs(text)

    input_ids = input_ids.to(config.device)
    attention_mask = attention_mask.to(config.device)

    output, y_hat = model(input_ids, attention_mask)

    if (text[index] == get_text(y_hat.argmax(-1))):
        correct = correct + 1

    total = total + 1

print(correct / total)

# correct = 0
# for x, y_truth in val_loader:
#   x, y_truth = x.to(config.device), y_truth.to(config.device)
#   output, y_hat = model(input_embeddings = x)
#   correct = correct + int(torch.sum(output.argmax(-1).squeeze(0) ==  y_truth.argmax(-1)))

# print(correct/len(val_loader.dataset))

total = 0
correct = 0
train_dataset.set_test_mode()

for i_batch, sample in enumerate(train_loader):
    x, y_truth = sample["data"].to(config.device), sample["gt"].to(config.device)

    text = get_text(y_truth.squeeze(0))
    input_ids, attention_mask, y_truth, index = get_inputs(text)

    input_ids = input_ids.to(config.device)
    attention_mask = attention_mask.to(config.device)

    output, y_hat = model(input_ids, attention_mask)

    # print("Text: ", text, index)
    # print("Label: ", text[index])
    # print("Prediction: ", get_text(y_truth.argmax(-1)))

    if (text[index] == get_text(y_truth.argmax(-1))):
        correct = correct + 1

    total = total + 1

print(correct / total)
