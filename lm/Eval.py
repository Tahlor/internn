# -*- coding: utf-8 -*-
"""CharBERTV3_BASE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOlk2rIdrl2_LSFyi1WZUa7Sc9Kn94qs


## Next step:
    # Why wasn't epoch loop working?
    # yhat vs ytruth, check for correctness

"""

# !pip install git+https://github.com/huggingface/transformers
# conda activate transformers
# from google.colab import drive
# drive.mount('/content/drive')
# import os
# os.chdir("/content/drive/My Drive/internn/data")
import torch.optim as optim
import copy
import numpy as np
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import os
from sen_loader import *
from transformers import BertPreTrainedModel
from transformers.models.bert.modeling_bert import *
import sys
from CustomBert import *
sys.path.append("..")
from pytorch_utils import *
from general_tools.my_logging import *
import wandb
import random
from error_measures import *

WANDB = False
if WANDB:
    wandb.init(project="TrainBERT")

ROOT = get_root("internn")
print(ROOT)
PATH = ROOT / "data/embedding_datasets/embeddings_v2"
EXPERIMENT_NAME = "BERT_logit*.py"
EXPERIMENT_NAME = "BERT_embedding*.py"

epochs = 10
starting_epoch = 0
lr = 1e-4
losses = []
losses_10 = []
vocab_size = 27
device = torch.device('cuda:0')
mask_one_token = True
text = 'abcdefghi jklmnopqrstuvwxyz'
corpus = [char for char in text]
mask_id = len(corpus) + 1
vocab_size = mask_id + 2 # some special characters?
batch_size = 1

config = {
    "epochs" : epochs,
    "starting_epoch" : starting_epoch,
    "lr" : lr,
    "losses" : losses,
    "losses_10" : losses_10,
    "vocab_size" : vocab_size,
    "device" : device,
    "mask_one_token": mask_one_token,
    "text" : text,
    "corpus" : corpus,
    "mask_id" : mask_id,
    "vocab_size" : vocab_size,
    "batch_size" : batch_size,
}

if WANDB:
    wandb.config = config

train_dataset = SentenceDataset(PATH=PATH / 'train_test_sentenceDataset.pt', which='Embeddings')
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)

test_dataset = SentenceDataset(PATH=PATH / 'train_test_sentenceDataset.pt', which='Embeddings', train=False)
test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)

sample = next(iter(train_dataset))
print("Batch Data Shape = ", sample["data"].squeeze(0).shape)
print("Num Batch Labels = ", sample["gt_one_hot"].squeeze(0).shape)
print("Output shape: ", sample["vgg_logits"].shape)
print("Sen Lengths: ", sample["length"])

print(get_text(sample["gt_one_hot"]))

model = BertModelCustom(BertConfig(vocab_size=vocab_size + 2)).to(device)
objective = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)

if PATH:
    latest = get_latest_file(PATH, EXPERIMENT_NAME)
    if latest:
        print(f"Loading {latest}")
        state_dict = load_model(latest, model, optimizer)

## TEST MODEL
# input_ids, attention_mask, labels, mask_index = get_inputs("my name is sam", corpus=corpus, mask_id=mask_id)
# output = model(input_ids.to(device), attention_mask.to(device))
X_KEY = "vgg_logits" if "logit" in EXPERIMENT_NAME else "data"

def eval_accuracy_single_letter(loader):
    pass


def eval_accuracy(loader, accuracy_measure="full sequence"):
    """
    Mask one letter and predict it

    accuracy_measure (str): how good is the LM, "single character"
                            how good is the whole system "full sequence"
    """
    wtd_sum = 0
    wt = 0
    helpful_cases = []
    for i_batch, sample in enumerate(loader):
        x, y_truth, attention_mask = sample[X_KEY].to(device), sample["masked_gt"].to(device), sample["attention_mask"].to(device)
        text = sample["text"][0].lower()
        input_ids = sample["gt_idxs"].to(device)
        output, y_hat = model(input_ids, attention_mask)
        wt+=sample["length"]
        wtd_sum+=cer(text, get_text(y_hat.argmax(-1)))*sample["length"]
        # helpful_cases = y_hat.argmax(-1),

    return wtd_sum/wt

wtd_avg_test = eval_accuracy(test_loader)
print("Test:", wtd_avg_test)
wtd_avg_train = eval_accuracy(train_loader)
print("Train:", wtd_avg_train)