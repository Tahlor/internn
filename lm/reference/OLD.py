# -*- coding: utf-8 -*-
"""CharBERTV3_BASE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOlk2rIdrl2_LSFyi1WZUa7Sc9Kn94qs


## Next step:
    # Why wasn't epoch loop working?
    # yhat vs ytruth, check for correctness

"""

# !pip install git+https://github.com/huggingface/transformers
# conda activate transformers
# from google.colab import drive
# drive.mount('/content/drive')
# import os
# os.chdir("/content/drive/My Drive/internn/data")
import torch.optim as optim
import copy
import numpy as np
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
import os
from sen_loader2 import *
from transformers import BertPreTrainedModel
from transformers.models.bert.modeling_bert import *
import sys
from CustomBert import *
sys.path.append("..")
from pytorch_utils import *
from general_tools.my_logging import *

ROOT = get_root("internn")
print(ROOT)
PATH = ROOT / "data/embedding_datasets/embeddings_v2"

text = 'abcdefghi jklmnopqrstuvwxyz'
corpus = [char for char in text]
mask_id = len(corpus) + 1
vocab_size = mask_id + 2

train_dataset = SentenceDataset(PATH=PATH / 'train_test_sentenceDataset.pt', which='Embeddings')
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)

test_dataset = SentenceDataset(PATH=PATH / 'train_test_sentenceDataset.pt', which='Embeddings', train=False)
test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)

def get_text(tnsr):
    text = ''
    for value in tnsr:
        if value.dim() and len(value) > 1:  # accepts either one-hot tensor OR one digit tensor
            value = torch.argmax(value)
        c = num_to_letter(value.item())
        text = text + c
    return text

sample = next(iter(train_dataset))
print("Batch Data Shape = ", sample[0].squeeze(0).shape) # torch.Size([31, 512]) sentence_len, embedding dim
print("Num Batch Labels = ", sample[1].squeeze(0).shape) # torch.Size([31, 27]) sentence_len, vocab_size
print("Output shape: ", sample[2].shape) # torch.Size([31, 27]) - distribution???
print("Sen Lengths: ", sample[3]) #

print(get_text(sample[1]))

def get_inputs(text, mask_index=None):
    n = len(text)
    if not mask_index:
        mask_index = np.random.randint(0, n)

    ids = []
    for char in text:
        ids.append(corpus.index(char))
    input_ids = torch.tensor(ids).unsqueeze(0)
    attention_mask = [1] * n
    attention_mask[mask_index] = 0
    attention_mask = torch.tensor(attention_mask).unsqueeze(0)

    labels = copy.deepcopy(input_ids)
    input_ids[0][mask_index] = mask_id
    labels[input_ids != mask_id] = -100
    labels = labels.type(torch.LongTensor)

    return input_ids, attention_mask, labels, mask_index


epochs = 5
starting_epoch = 0
lr = 5e-5
losses = []
losses_10 = []
vocab_size = 27
device = torch.device('cuda:0')

model = BertModelCustom(BertConfig(vocab_size=vocab_size + 2)).to(device)
objective = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)

# if PATH:
#     latest = get_latest_file(PATH)
#     print(f"Loading {latest}")
#     model, optimizer, starting_epoch, loss = load_model(latest, model, optimizer)
#     starting_epoch = 1

## TEST MODEL
input_ids, attention_mask, labels, mask_index = get_inputs("my name is sam")
output = model(input_ids.to(device), attention_mask.to(device))

prev_input = None
cur_input = None
prev_loss = 100
cur_loss = 100
prev_output = None
cur_output = None
val_loss = []
count = 0

for epoch in range(starting_epoch, epochs):
    print("epoch", epoch)
    for i_batch, sample in enumerate(train_loader):
        x, y_truth = sample[0].to(device), sample[1].to(device)
        text = get_text(y_truth.squeeze(0))
        input_ids, attention_mask, y_truth, index = get_inputs(text)
        # 1,31 * 3; int; indexes, 1/0s, label_masks (-100's mostly)
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)

        optimizer.zero_grad()
        output, y_hat = model(input_ids, attention_mask)

        loss = objective(output.view(-1, vocab_size), y_truth.squeeze(0).to(device))

        loss.backward()
        losses_10.append(loss.item())
        optimizer.step()

        count = count + 1
        if count % 100 == 0:
            losses.append(np.average(losses_10))
            losses_10 = []
            print("count", count)
    save_model(incrementer(PATH, "BERT.pt"), model, optimizer, epoch=epoch, loss=losses[-1] if losses else 0)

import matplotlib.pyplot as plt

plt.plot(losses)

total = 0
correct = 0

"""
Mask one letter and predict it
"""
for i_batch, sample in enumerate(train_loader):
    x, y_truth = sample[0].to(device), sample[1].to(device)

    text = get_text(y_truth.squeeze(0))
    input_ids, attention_mask, y_truth, index = get_inputs(text)

    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    output, y_hat = model(input_ids, attention_mask)

    if (text[index] == get_text(y_hat.argmax(-1))):
        correct = correct + 1

    total = total + 1

print(correct / total)

# correct = 0
# for x, y_truth in val_loader:
#   x, y_truth = x.to(device), y_truth.to(device)
#   output, y_hat = model(input_embeddings = x)
#   correct = correct + int(torch.sum(output.argmax(-1).squeeze(0) ==  y_truth.argmax(-1)))

# print(correct/len(val_loader.dataset))

total = 0
correct = 0
for i_batch, sample in enumerate(test_loader):
    x, y_truth = sample[0].to(device), sample[1].to(device)

    text = get_text(y_truth.squeeze(0))
    input_ids, attention_mask, y_truth, index = get_inputs(text)

    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    output, y_hat = model(input_ids, attention_mask)

    # print("Text: ", text, index)
    # print("Label: ", text[index])
    # print("Prediction: ", get_text(y_truth.argmax(-1)))

    if (text[index] == get_text(y_truth.argmax(-1))):
        correct = correct + 1

    total = total + 1

print(correct / total)
