{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharBert",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Rxrio2Ijoo",
        "outputId": "85609e69-51ae-4d7b-ef31-e37895b87aa6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vz5FYfJIsrg"
      },
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import NFD, StripAccents\n",
        "from tokenizers import Tokenizer, Regex, NormalizedString, PreTokenizedString\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import PreTokenizer\n",
        "from tokenizers.normalizers import Normalizer\n",
        "from tokenizers.decoders import Decoder"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k47hIoT0Iw4T",
        "outputId": "21180686-7f78-477a-e3db-90511d3b38dd"
      },
      "source": [
        "#The dataset\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-08 16:08:26--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.70.142\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.70.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘wikitext-103-raw-v1.zip.1’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  43.3MB/s    in 4.7s    \n",
            "\n",
            "2021-05-08 16:08:31 (39.1 MB/s) - ‘wikitext-103-raw-v1.zip.1’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "replace wikitext-103-raw/wiki.test.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace wikitext-103-raw/wiki.valid.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace wikitext-103-raw/wiki.train.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snXvvkaVrz80"
      },
      "source": [
        "Build the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4_2ERcMI2wo"
      },
      "source": [
        "from tokenizers import Tokenizer, Regex, NormalizedString, PreTokenizedString\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import PreTokenizer\n",
        "from tokenizers.normalizers import Normalizer, Lowercase, NFD, StripAccents\n",
        "from tokenizers.decoders import Decoder\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "\n",
        "class CharPreTokenizer:\n",
        "    def char_split(self, i: int, normalized_string: NormalizedString):\n",
        "        splits = []\n",
        "\n",
        "        for i in range(0, len(str(normalized_string))):\n",
        "            splits.append(normalized_string[i:i+1])\n",
        "        return splits\n",
        "\n",
        "\n",
        "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
        "        pretok.split(self.char_split)\n",
        "\n",
        "\n",
        "\n",
        "class CustomNormalizer:\n",
        "    def normalize(self, normalized: NormalizedString):\n",
        "        normalized.nfkc()\n",
        "        normalized.filter(lambda char: not char.isnumeric())\n",
        "        normalized.replace(Regex(\"\\s+\"), \" \")\n",
        "        normalized.lowercase()\n",
        "\n",
        "\n",
        "tok = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "tok.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "tok.pre_tokenizer = PreTokenizer.custom(CharPreTokenizer())\n",
        "\n",
        "tok.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLW5LXzw3Exv"
      },
      "source": [
        "Create a small txt file to train on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onsW9Zwk0sBU"
      },
      "source": [
        "with open('MM_Chp_1.txt', 'r') as file:\n",
        "    data = file.read().replace('\\n', '')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb74G8ou1EKO"
      },
      "source": [
        "data = data.lower()\n",
        "chars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' ']\n",
        "\n",
        "data = ''.join([i for i in data if i in chars])\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "SUHZzjwP2YLM",
        "outputId": "a804ad4a-e668-4cb9-b868-cafab8f42a69"
      },
      "source": [
        "data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'miss brooke had that kind of beauty which seems to be thrown into relief by poor dress her hand and wrist were so finely formed that she could wear sleeves not less bare of style than those in which the blessed virgin appeared to italian painters and her profile as well as her stature and bearing seemed to gain the more dignity from her plain garments which by the side of provincial fashion gave her the impressiveness of a fine quotation from the bibleor from one of our elder poetsin a paragraph of todays newspaper she was usually spoken of as being remarkably clever but with the addition that her sister celia had more commonsense nevertheless celia wore scarcely more trimmings and it was only to close observers that her dress differed from her sisters and had a shade of coquetry in its arrangements for miss brookes plain dressing was due to mixed conditions in most of which her sister shared the pride of being ladies had something to do with it the brooke connections though not exactly aristocratic were unquestionably good if you inquired backward for a generation or two you would not find any yardmeasuring or parceltying forefathersanything lower than an admiral or a clergyman and there was even an ancestor discernible as a puritan gentleman who served under cromwell but afterwards conformed and managed to come out of all political troubles as the proprietor of a respectable family estate young women of such birth living in a quiet countryhouse and attending a village church hardly larger than a parlor naturally regarded frippery as the ambition of a hucksters daughter then there was wellbred economy which in those days made show in dress the first item to be deducted from when any margin was required for expenses more distinctive of rank such reasons would have been enough to account for plain dress quite apart from religious feeling but in miss brookes case religion alone would have determined it and celia mildly acquiesced in all her sisters sentiments only infusing them with that commonsense which is able to accept momentous doctrines without any eccentric agitation dorothea knew many passages of pascals pensees and of jeremy taylor by heart and to her the destinies of mankind seen by the light of christianity made the solicitudes of feminine fashion appear an occupation for bedlam she could not reconcile the anxieties of a spiritual life involving eternal consequences with a keen interest in gimp and artificial protrusions of drapery her mind was theoretic and yearned by its nature after some lofty conception of the world which might frankly include the parish of tipton and her own rule of conduct there she was enamoured of intensity and greatness and rash in embracing whatever seemed to her to have those aspects likely to seek martyrdom to make retractations and then to incur martyrdom after all in a quarter where she had not sought it certainly such elements in the character of a marriageable girl tended to interfere with her lot and hinder it from being decided according to custom by good looks vanity and merely canine affection with all this she the elder of the sisters was not yet twenty and they had both been educated since they were about twelve years old and had lost their parents on plans at once narrow and promiscuous first in an english family and afterwards in a swiss family at lausanne their bachelor uncle and guardian trying in this way to remedy the disadvantages of their orphaned conditionit was hardly a year since they had come to live at tipton grange with their uncle a man nearly sixty of acquiescent temper miscellaneous opinions and uncertain vote he had travelled in his younger years and was held in this part of the county to have contracted a too rambling habit of mind mr brookes conclusions were as difficult to predict as the weather it was only safe to say that he would act with benevolent intentions and that he would spend as little money as possible in carrying them out for the most glutinously indefinite minds enclose some hard grains of habit and a man has been seen lax about all his own interests except the retention of his snuffbox concerning which he was watchful suspicious and greedy of clutchin mr brooke the hereditary strain of puritan energy was clearly in abeyance but in his niece dorothea it glowed alike through faults and virtues turning sometimes into impatience of her uncles talk or his way of letting things be on his estate and making her long all the more for the time when she would be of age and have some command of money for generous schemes she was regarded as an heiress for not only had the sisters seven hundred ayear each from their parents but if dorothea married and had a son that son would inherit mr brookes estate presumably worth about three thousand ayeara rental which seemed wealth to provincial families still discussing mr peels late conduct on the catholic question innocent of future goldfields and of that gorgeous plutocracy which has so nobly exalted the necessities of genteel lifeand how should dorothea not marrya girl so handsome and with such prospects nothing could hinder it but her love of extremes and her insistence on regulating life according to notions which might cause a wary man to hesitate before he made her an offer or even might lead her at last to refuse all offers a young lady of some birth and fortune who knelt suddenly down on a brick floor by the side of a sick laborer and prayed fervidly as if she thought herself living in the time of the apostleswho had strange whims of fasting like a papist and of sitting up at night to read old theological books such a wife might awaken you some fine morning with a new scheme for the application of her income which would interfere with political economy and the keeping of saddlehorses a man would naturally think twice before he risked himself in such fellowship women were expected to have weak opinions but the great safeguard of society and of domestic life was that opinions were not acted on sane people did what their neighbors did so that if any lunatics were at large one might know and avoid themthe rural opinion about the new young ladies even among the cottagers was generally in favor of celia as being so amiable and innocentlooking while miss brookes large eyes seemed like her religion too unusual and striking poor dorothea compared with her the innocentlooking celia was knowing and worldlywise so much subtler is a human mind than the outside tissues which make a sort of blazonry or clockface for ityet those who approached dorothea though prejudiced against her by this alarming hearsay found that she had a charm unaccountably reconcilable with it most men thought her bewitching when she was on horseback she loved the fresh air and the various aspects of the country and when her eyes and cheeks glowed with mingled pleasure she looked very little like a devotee riding was an indulgence which she allowed herself in spite of conscientious qualms she felt that she enjoyed it in a pagan sensuous way and always looked forward to renouncing itshe was open ardent and not in the least selfadmiring indeed it was pretty to see how her imagination adorned her sister celia with attractions altogether superior to her own and if any gentleman appeared to come to the grange from some other motive than that of seeing mr brooke she concluded that he must be in love with celia sir james chettam for example whom she constantly considered from celias point of view inwardly debating whether it would be good for celia to accept him that he should be regarded as a suitor to herself would have seemed to her a ridiculous irrelevance dorothea with all her eagerness to know the truths of life retained very childlike ideas about marriage she felt sure that she would have accepted the judicious hooker if she had been born in time to save him from that wretched mistake he made in matrimony or john milton when his blindness had come on or any of the other great men whose odd habits it would have been glorious piety to endure but an amiable handsome baronet who said exactly to her remarks even when she expressed uncertaintyhow could he affect her as a lover the really delightful marriage must be that where your husband was a sort of father and could teach you even hebrew if you wished itthese peculiarities of dorotheas character caused mr brooke to be all the more blamed in neighboring families for not securing some middleaged lady as guide and companion to his nieces but he himself dreaded so much the sort of superior woman likely to be available for such a position that he allowed himself to be dissuaded by dorotheas objections and was in this case brave enough to defy the worldthat is to say mrs cadwallader the rectors wife and the small group of gentry with whom he visited in the northeast corner of loamshire so miss brooke presided in her uncles household and did not at all dislike her new authority with the homage that belonged to itsir james chettam was going to dine at the grange today with another gentleman whom the girls had never seen and about whom dorothea felt some venerating expectation this was the reverend edward casaubon noted in the county as a man of profound learning understood for many years to be engaged on a great work concerning religious history also as a man of wealth enough to give lustre to his piety and having views of his own which were to be more clearly ascertained on the publication of his book his very name carried an impressiveness hardly to be measured without a precise chronology of scholarshipearly in the day dorothea had returned from the infant school which she had set going in the village and was taking her usual place in the pretty sittingroom which divided the bedrooms of the sisters bent on finishing a plan for some buildings a kind of work which she delighted in when celia who had been watching her with a hesitating desire to propose something saiddorothea dear if you dont mindif you are not very busysuppose we looked at mammas jewels today and divided them it is exactly six months today since uncle gave them to you and you have not looked at them yetcelias face had the shadow of a pouting expression in it the full presence of the pout being kept back by an habitual awe of dorothea and principle two associated facts which might show a mysterious electricity if you touched them incautiously to her relief dorotheas eyes were full of laughter as she looked upwhat a wonderful little almanac you are celia is it six calendar or six lunar monthsit is the last day of september now and it was the first of april when uncle gave them to you you know he said that he had forgotten them till then i believe you have never thought of them since you locked them up in the cabinet herewell dear we should never wear them you know dorothea spoke in a full cordial tone half caressing half explanatory she had her pencil in her hand and was making tiny sideplans on a margincelia colored and looked very grave i think dear we are wanting in respect to mammas memory to put them by and take no notice of them and she added after hesitating a little with a rising sob of mortification necklaces are quite usual now and madame poincon who was stricter in some things even than you are used to wear ornaments and christians generallysurely there are women in heaven now who wore jewels celia was conscious of some mental strength when she really applied herself to argumentyou would like to wear them exclaimed dorothea an air of astonished discovery animating her whole person with a dramatic action which she had caught from that very madame poincon who wore the ornaments of course then let us have them out why did you not tell me before but the keys the keys she pressed her hands against the sides of her head and seemed to despair of her memorythey are here said celia with whom this explanation had been long meditated and prearrangedpray open the large drawer of the cabinet and get out the jewelboxthe casket was soon open before them and the various jewels spread out making a bright parterre on the table it was no great collection but a few of the ornaments were really of remarkable beauty the finest that was obvious at first being a necklace of purple amethysts set in exquisite gold work and a pearl cross with five brilliants in it dorothea immediately took up the necklace and fastened it round her sisters neck where it fitted almost as closely as a bracelet but the circle suited the henriettamaria style of celias head and neck and she could see that it did in the pierglass oppositethere celia you can wear that with your indian muslin but this cross you must wear with your dark dressescelia was trying not to smile with pleasure o dodo you must keep the cross yourselfno no dear no said dorothea putting up her hand with careless deprecationyes indeed you must it would suit youin your black dress now said celia insistingly you might wear thatnot for the world not for the world a cross is the last thing i would wear as a trinket dorothea shuddered slightlythen you will think it wicked in me to wear it said celia uneasilyno dear no said dorothea stroking her sisters cheek souls have complexions too what will suit one will not suit anotherbut you might like to keep it for mammas sakeno i have other things of mammasher sandalwood box which i am so fond ofplenty of things in fact they are all yours dear we need discuss them no longer theretake away your propertycelia felt a little hurt there was a strong assumption of superiority in this puritanic toleration hardly less trying to the blond flesh of an unenthusiastic sister than a puritanic persecutionbut how can i wear ornaments if you who are the elder sister will never wear themnay celia that is too much to ask that i should wear trinkets to keep you in countenance if i were to put on such a necklace as that i should feel as if i had been pirouetting the world would go round with me and i should not know how to walkcelia had unclasped the necklace and drawn it off it would be a little tight for your neck something to lie down and hang would suit you better she said with some satisfaction the complete unfitness of the necklace from all points of view for dorothea made celia happier in taking it she was opening some ringboxes which disclosed a fine emerald with diamonds and just then the sun passing beyond a cloud sent a bright gleam over the tablehow very beautiful these gems are said dorothea under a new current of feeling as sudden as the gleam it is strange how deeply colors seem to penetrate one like scent i suppose that is the reason why gems are used as spiritual emblems in the revelation of st john they look like fragments of heaven i think that emerald is more beautiful than any of themand there is a bracelet to match it said celia we did not notice this at firstthey are lovely said dorothea slipping the ring and bracelet on her finely turned finger and wrist and holding them towards the window on a level with her eyes all the while her thought was trying to justify her delight in the colors by merging them in her mystic religious joyyou would like those dorothea said celia rather falteringly beginning to think with wonder that her sister showed some weakness and also that emeralds would suit her own complexion even better than purple amethysts you must keep that ring and braceletif nothing else but see these agates are very pretty and quietyes i will keep thesethis ring and bracelet said dorothea then letting her hand fall on the table she said in another toneyet what miserable men find such things and work at them and sell them she paused again and celia thought that her sister was going to renounce the ornaments as in consistency she ought to doyes dear i will keep these said dorothea decidedly but take all the rest away and the casketshe took up her pencil without removing the jewels and still looking at them she thought of often having them by her to feed her eye at these little fountains of pure colorshall you wear them in company said celia who was watching her with real curiosity as to what she would dodorothea glanced quickly at her sister across all her imaginative adornment of those whom she loved there darted now and then a keen discernment which was not without a scorching quality if miss brooke ever attained perfect meekness it would not be for lack of inward fireperhaps she said rather haughtily i cannot tell to what level i may sinkcelia blushed and was unhappy she saw that she had offended her sister and dared not say even anything pretty about the gift of the ornaments which she put back into the box and carried away dorothea too was unhappy as she went on with her plandrawing questioning the purity of her own feeling and speech in the scene which had ended with that little explosioncelias consciousness told her that she had not been at all in the wrong it was quite natural and justifiable that she should have asked that question and she repeated to herself that dorothea was inconsistent either she should have taken her full share of the jewels or after what she had said she should have renounced them altogetheri am sureat least i trust thought celia that the wearing of a necklace will not interfere with my prayers and i do not see that i should be bound by dorotheas opinions now we are going into society though of course she herself ought to be bound by them but dorothea is not always consistentthus celia mutely bending over her tapestry until she heard her sister calling herhere kitty come and look at my plan i shall think i am a great architect if i have not got incompatible stairs and fireplacesas celia bent over the paper dorothea put her cheek against her sisters arm caressingly celia understood the action dorothea saw that she had been in the wrong and celia pardoned her since they could remember there had been a mixture of criticism and awe in the attitude of celias mind towards her elder sister the younger had always worn a yoke but is there any yoked creature without its private opinions'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x53WY_Na2zDZ"
      },
      "source": [
        "text_file = open(\"sample.txt\", \"w\")\n",
        "n = text_file.write(data)\n",
        "text_file.close()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5r8oQjgJkZM"
      },
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "#We may want to look into this again\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=27, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[PAD]\"]\n",
        ")\n",
        "files = ['sample.txt']\n",
        "tok.train(files, trainer)\n",
        "tok.pre_tokenizer = Whitespace()\n",
        "\n",
        "tok.save(\"sample.json\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdoaR-lbcMEE"
      },
      "source": [
        "tok.pre_tokenizer = PreTokenizer.custom(CharPreTokenizer())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeEJbAUCLgEF",
        "outputId": "d0fbac6b-50d0-4a0f-e2cc-5d37dab27abd"
      },
      "source": [
        "output = tok.encode(\"My name is John\")\n",
        "print(output.tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY2ZlTD5r7PL"
      },
      "source": [
        "Load the tokenizer from json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-mlK-gALnlt"
      },
      "source": [
        "new_tok = Tokenizer.from_file(\"sample.json\")\n",
        "# new_tok.add_special_tokens({'pad_token': '[PAD]'})\n",
        "new_tok.pre_tokenizer = PreTokenizer.custom(CharPreTokenizer())\n",
        "new_tok.enable_truncation(max_length=100)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7mTHYEJefvR",
        "outputId": "bd415c49-ed77-4786-e0fe-39227559321d"
      },
      "source": [
        "new_tok.encode(\"I have a dog named Spot\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjldzlQKdoAS",
        "outputId": "15219384-2af4-4de5-f7ac-94ba39ae05d4"
      },
      "source": [
        "output = new_tok.encode(\"I have a dog named Spot\")\n",
        "print(output.tokens)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'i', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'd', 'o', 'g', ' ', 'n', 'a', 'm', 'e', 'd', ' ', 's', 'p', 'o', 't', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBuL25lfbPs"
      },
      "source": [
        "Now Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiGNotCUey43",
        "outputId": "c40cc9d0-10fd-4619-b3bb-cc3605742f41"
      },
      "source": [
        "# Check that we have a GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 16:44:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp0-vhkyf5yC",
        "outputId": "65458409-1b8d-44d1-ba03-9b385d651a44"
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRhIaRHxqFal"
      },
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_object=new_tok)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6URc178f5_j",
        "outputId": "a601d6ce-5241-42f7-b65f-f9d7e728aa79"
      },
      "source": [
        "from transformers import DistilBertForMaskedLM, DistilBertConfig\n",
        "\n",
        "#may want to look at hidden_dim\n",
        "configuration = DistilBertConfig(\n",
        "    vocab_size=27,\n",
        "    max_position_embeddings=100,\n",
        "    n_heads = 6,\n",
        "    n_layers = 3, \n",
        ")\n",
        "\n",
        "model = DistilBertForMaskedLM(configuration)\n",
        "model.num_parameters()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21954843"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tld8NfaRhILI",
        "outputId": "8f999850-c464-448a-c5b7-ade30d131f8c"
      },
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"sample.txt\",\n",
        "    block_size=16,\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.01 s, sys: 354 ms, total: 2.36 s\n",
            "Wall time: 3.76 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAON5Bm84yEx"
      },
      "source": [
        "tokenizer.mask_token = \"[MASK]\"\n",
        "tokenizer.pad_token = \"[PAD]\""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt2EDpg7hWdC"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN1tb1P746zb"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_tok\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "b3tJshhZ6F1M",
        "outputId": "bdde88d9-f36c-4067-c6c6-76fdb1f06600"
      },
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 501 ms, sys: 242 ms, total: 742 ms\n",
            "Wall time: 616 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=3.2399353981018066, metrics={'train_runtime': 0.3394, 'train_samples_per_second': 2.946, 'total_flos': 2107664928.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 87819776, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 10530816, 'train_mem_gpu_alloc_delta': 263461376, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 13884416})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCC4dwE76GIJ"
      },
      "source": [
        "trainer.save_model(\"./bert_tok\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWt0SE3Y7YCO"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./bert_tok\",\n",
        "    tokenizer=\"./bert_tok\"\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RgzrsMg7gDM",
        "outputId": "0e0f39f6-4055-4eb0-e96f-51c7a5bad57e"
      },
      "source": [
        "fill_mask(\"[MASK]\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2431497424840927, 'sequence': '', 'token': 5, 'token_str': ''},\n",
              " {'score': 0.05928969383239746,\n",
              "  'sequence': '2',\n",
              "  'token': 25,\n",
              "  'token_str': '2'},\n",
              " {'score': 0.058045774698257446,\n",
              "  'sequence': '.',\n",
              "  'token': 21,\n",
              "  'token_str': '.'},\n",
              " {'score': 0.04953441023826599, 'sequence': ' ', 'token': 7, 'token_str': ' '},\n",
              " {'score': 0.04010901600122452,\n",
              "  'sequence': '',\n",
              "  'token': 2,\n",
              "  'token_str': '[SEP]'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KmM_TwJ7pUq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}